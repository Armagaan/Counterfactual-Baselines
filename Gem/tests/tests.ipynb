{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring\n",
    "Exploring what the outputs of various scripts represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of the evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Comments represent corresponding names in test_explained_adj.py\n",
    "# merged_correct\n",
    "merged_correct = pd.read_csv(\n",
    "    \"../log/syn1_top6/acc.csv\",\n",
    "    header = None,\n",
    "    names = [\"Node\", \"0\", \"1\", \"2\", \"3\"]\n",
    ")\n",
    "\n",
    "# merged_odd\n",
    "merged_odd = pd.read_csv(\n",
    "    \"../log/syn1_top6/log_odd.csv\",\n",
    "    header = None\n",
    ")\n",
    "\n",
    "# merged_loss\n",
    "merged_loss = pd.read_csv(\n",
    "    \"../log/syn1_top6/loss.csv\",\n",
    "    header = None,\n",
    "    names = [\"Node\", \"0\", \"1\", \"2\", \"3\"]\n",
    ")\n",
    "\n",
    "# pred_prob\n",
    "pred_prob = pd.read_csv(\n",
    "    \"../log/syn1_top6/prob.csv\",\n",
    "    header = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "merged_correct:   (34, 5)\n",
      "merged_odd:       (4, 34)\n",
      "merged_loss:      (34, 5)\n",
      "pred_prob:        (34, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Shapes:\\n\"\n",
    "    f\"merged_correct:   {merged_correct.shape}\\n\"\n",
    "    f\"merged_odd:       {merged_odd.shape}\\n\"\n",
    "    f\"merged_loss:      {merged_loss.shape}\\n\"\n",
    "    f\"pred_prob:        {pred_prob.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"../explanation/syn1_top6\")\n",
    "explanation_filenames = sorted([file for file in filenames if 'label' in file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = dict()\n",
    "for filename in explanation_filenames:\n",
    "    explanation = pd.read_csv(f\"../explanation/syn1_top6/{filename}\", header=None).to_numpy()\n",
    "    # filename[4:7] is the target node_id.\n",
    "    explanations[int(filename[4:7])] = explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be extracted from pred_proba\n",
    "base = [1,2,3,4]\n",
    "proba_columns_original = [0] + [i for i in base]\n",
    "proba_columns_gem = [0] + [8 + i for i in base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting relevant data into separate dataframes\n",
    "pred_proba_original = pred_prob[proba_columns_original].copy()\n",
    "pred_proba_gem = pred_prob[proba_columns_gem].copy()\n",
    "\n",
    "# rename columns to [node_id, 0,1,2,3] in both dataframes\n",
    "columns = [\"node_id\", 0,1,2,3]\n",
    "pred_proba_original.rename(\n",
    "    columns = {old : new for old, new in zip(pred_proba_original, columns)},\n",
    "    inplace = True\n",
    ")\n",
    "pred_proba_gem.rename(\n",
    "    columns = {old : new for old, new in zip(pred_proba_gem, columns)},\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions based on probability\n",
    "predictions_original = {\n",
    "    int(key):value for key, value in zip(\n",
    "        pred_proba_original[\"node_id\"],\n",
    "        np.argmax(\n",
    "            pred_proba_original.drop(\"node_id\", axis=1).to_numpy(),\n",
    "            axis = -1\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "predictions_gem = {\n",
    "    int(key):value for key, value in zip(\n",
    "        pred_proba_gem[\"node_id\"],\n",
    "        np.argmax(\n",
    "            pred_proba_gem.drop(\"node_id\", axis=1).to_numpy(),\n",
    "            axis = -1\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmerged_loss = np.stack([\\n        valid_node_idxs,\\n        org_losses,\\n        extracted_losses,\\n        ours_losses,\\n        gnnexp_losses\\n    ], axis=1)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "merged_loss = np.stack([\n",
    "        valid_node_idxs,\n",
    "        org_losses,\n",
    "        extracted_losses,\n",
    "        ours_losses,\n",
    "        gnnexp_losses\n",
    "    ], axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmerged_correct = np.stack([\\n    valid_node_idxs,\\n    org_corrects,\\n    extracted_corrects,\\n    ours_corrects,\\n    gnnexp_corrects\\n    ], axis=1)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "merged_correct = np.stack([\n",
    "    valid_node_idxs,\n",
    "    org_corrects,\n",
    "    extracted_corrects,\n",
    "    ours_corrects,\n",
    "    gnnexp_corrects\n",
    "    ], axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npred_prob += [[node_idx] + list(org_p) + list(extracted_p) + list(ours_p) + list(gnnexp_p)]\\npred_prob = np.stack(pred_prob, axis=0)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pred_prob += [[node_idx] + list(org_p) + list(extracted_p) + list(ours_p) + list(gnnexp_p)]\n",
    "pred_prob = np.stack(pred_prob, axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmerged_odd = np.stack([\\n        our_ground_truth_odd,\\n        gnnexp_ground_truth_odd,\\n        our_pred_label_odd,\\n        gnnexp_pred_label_odd\\n    ])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "merged_odd = np.stack([\n",
    "        our_ground_truth_odd,\n",
    "        gnnexp_ground_truth_odd,\n",
    "        our_pred_label_odd,\n",
    "        gnnexp_pred_label_odd\n",
    "    ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nour_ground_truth_odd = []\\nour_ground_truth_odd += [log_odd(org_p[node_label]) - log_odd(ours_p[node_label])]\\n\\nour_pred_label_odd = []\\nour_pred_label_odd += [log_odd(org_p[org_pred]) - log_odd(ours_p[org_pred])]\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "our_ground_truth_odd = []\n",
    "our_ground_truth_odd += [log_odd(org_p[node_label]) - log_odd(ours_p[node_label])]\n",
    "\n",
    "our_pred_label_odd = []\n",
    "our_pred_label_odd += [log_odd(org_p[org_pred]) - log_odd(ours_p[org_pred])]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\norg_pred, org_p = evaluate_adj(\\n            node_idx_new,\\n            sub_feat,\\n            org_adj,\\n            sub_label,\\n            org_losses,\\n            org_corrects\\n        )\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "org_pred, org_p = evaluate_adj(\n",
    "            node_idx_new,\n",
    "            sub_feat,\n",
    "            org_adj,\n",
    "            sub_label,\n",
    "            org_losses,\n",
    "            org_corrects\n",
    "        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nours_pred, ours_p = evaluate_adj(\\n            node_idx_new,\\n            sub_feat,\\n            ours_adj,\\n            sub_label,\\n            ours_losses,\\n            ours_corrects\\n        )\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ours_pred, ours_p = evaluate_adj(\n",
    "            node_idx_new,\n",
    "            sub_feat,\n",
    "            ours_adj,\n",
    "            sub_label,\n",
    "            ours_losses,\n",
    "            ours_corrects\n",
    "        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_size = list()\n",
    "for explanation in explanations.values():\n",
    "    explanation_size.append(np.sum(explanation - np.eye(explanation.shape[0])) / 2)\n",
    "explanation_size = f\"{np.mean(explanation_size):.2f} +- {np.std(explanation_size):.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation size: 5.97 +- 0.17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Explanation size: {explanation_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per label Explanation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_label_explanation_size = defaultdict(list)\n",
    "for node_id, explanation in explanations.items():\n",
    "    label = predictions_original[node_id]\n",
    "    explanation_size = np.sum(explanation - np.eye(explanation.shape[0])) / 2\n",
    "    per_label_explanation_size[f\"label{label}\"].append(explanation_size)\n",
    "\n",
    "for label in range(4):\n",
    "    if len(per_label_explanation_size[f\"label{label}\"]) == 0:\n",
    "        per_label_explanation_size[f\"label{label}\"] = \"NA\"\n",
    "    else:    \n",
    "        per_label_explanation_size[f\"label{label}\"] = (\n",
    "            f\"{np.mean(per_label_explanation_size[f'label{label}']):.2f}\"\n",
    "            f\" +- {np.std(per_label_explanation_size[f'label{label}']):.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'label1': '5.94 +- 0.23',\n",
       "             'label2': '6.00 +- 0.00',\n",
       "             'label3': '6.00 +- 0.00',\n",
       "             'label0': 'NA'})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_label_explanation_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find diffrences between predictions\n",
    "differences = sum(predictions_original != predictions_gem)\n",
    "fidelity = differences/predictions_gem.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity: 0.06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fidelity: {fidelity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Label Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_label_fidelity = defaultdict(list)\n",
    "\n",
    "for (__, proba_original), (__, proba_gem) in \\\n",
    "        zip(pred_proba_original.iterrows(), pred_proba_gem.iterrows()):\n",
    "    prediction_original = np.argmax(proba_original)\n",
    "    prediction_gem = np.argmax(proba_gem)\n",
    "    if prediction_original != prediction_gem:\n",
    "        per_label_fidelity[f\"label{prediction_original}\"].append(1)\n",
    "    else:\n",
    "        per_label_fidelity[f\"label{prediction_original}\"].append(0)\n",
    "\n",
    "for label in range(4):\n",
    "    if len(per_label_fidelity[f\"label{label}\"]) == 0:\n",
    "        per_label_fidelity[f\"label{label}\"] = f\"NA\"\n",
    "    else:    \n",
    "        per_label_fidelity[f\"label{label}\"] = f\"{np.mean(per_label_fidelity[f'label{label}']):.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per label fidelity:\n",
      "defaultdict(<class 'list'>, {'label1': '0.11', 'label2': '0.00', 'label3': '0.00', 'label0': 'NA'})\n"
     ]
    }
   ],
   "source": [
    "print(\"Per label fidelity:\")\n",
    "print(per_label_fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6cbbe528126ce4ff858253ebeac4791d5498c577849f0c2b8ed17d9a06b9b755"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('gem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
